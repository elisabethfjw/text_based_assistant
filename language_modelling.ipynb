{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a language model to compute probabilities for individual English sentences.\n",
    "\n",
    "Implement a bigram language model as described in the lecture, and use it to compute the probability of a short sentence.\n",
    "What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? And what happens if your sentence is very long (e.g. 100 words or more)? Optionally, change your code so that it can handle these challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        words = corpus.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            current_word, next_word = words[i], words[i + 1]\n",
    "            self.bigram_counts[current_word][next_word] += 1\n",
    "            self.unigram_counts[current_word] += 1\n",
    "        # Handle the last word\n",
    "        self.unigram_counts[words[-1]] += 1\n",
    "\n",
    "    def calculate_probability(self, current_word, next_word):\n",
    "        if current_word in self.bigram_counts and next_word in self.bigram_counts[current_word]:\n",
    "            # Estimate probability using MLE\n",
    "            return self.bigram_counts[current_word][next_word] / self.unigram_counts[current_word]\n",
    "        else:\n",
    "            # Unseen word\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training corpus from the .sv file\n",
    "training_data_path = 'data/europarl-v7.sv-en.lc.en'\n",
    "\n",
    "with open(training_data_path, 'r', encoding='utf-8') as file:\n",
    "    training_data = file.read()\n",
    "\n",
    "# Create a bigram language model\n",
    "bigram_model = BigramLanguageModel()\n",
    "\n",
    "# Train the model\n",
    "bigram_model.train(training_data.lower())  # Change to lowercase for case-insensitive matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of short_sentence is: 1.3991701237103848e-09\n",
      "The probability of long_sentence is: 4.40313841760489e-25\n",
      "The probability of unseen_sentence is: 0.0\n"
     ]
    }
   ],
   "source": [
    "short_sentence = \"it is the case of alexander nikitin .\"\n",
    "\n",
    "long_sentence = \"\"\"indeed , it is quite in keeping with the positions this house has always adopted .\"\"\"\n",
    "\n",
    "unseen_sentence = \"the happy dog .\"\n",
    "\n",
    "\n",
    "\n",
    "for sentence, sentence_name in zip([short_sentence, long_sentence, unseen_sentence], [\"short_sentence\", \"long_sentence\", \"unseen_sentence\"]):\n",
    "    words = sentence.lower().split()\n",
    "    sentence_probability = 1.0\n",
    "    for i in range(len(words) - 1):\n",
    "        current_word, next_word = words[i], words[i + 1]\n",
    "        # Markov assumption: P(w1, w2, ..., wn) = P(w1) * P(w2 | w1) * P(w3 | w2) * ... * P(wn | wn-1)\n",
    "        sentence_probability *= bigram_model.calculate_probability(current_word, next_word)\n",
    "    print(f\"The probability of {sentence_name} is: {sentence_probability}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
